{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Assignment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Decision Tree Classifier Algorithm\n",
    "#A Decision Tree Classifier is a supervised learning algorithm used for classification tasks. It works by \n",
    "# recursively splitting the dataset based on feature values to create a tree-like model that predicts the target class.\n",
    "\n",
    "# How a Decision Tree Works:\n",
    "\n",
    "# Start with the Entire Dataset:\n",
    "# The root node contains all the training data.\n",
    "\n",
    "# Feature Selection & Splitting:\n",
    "# The algorithm selects the best feature to split the data using a splitting criterion like:\n",
    "    # Gini Impurity (default in Scikit-Learn)\n",
    "    # Entropy (Information Gain)\n",
    "# The data is divided into subsets, forming child nodes.\n",
    "\n",
    "# Recursive Partitioning:\n",
    "# The process repeats on each child node, splitting further based on other feature values.\n",
    "\n",
    "# Stopping Conditions:\n",
    "# The tree stops growing when:\n",
    "# All instances in a node belong to the same class.\n",
    "# A stopping condition is met (e.g., max depth, min samples per leaf).\n",
    "\n",
    "# Making Predictions:\n",
    "# For a new data point, the decision tree traverses from the root to a leaf node based on feature values.\n",
    "# The leaf node contains the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# A Decision Tree Classifier splits data based on features to create a tree-like model. It uses measures like \n",
    "# Gini Impurity or Entropy to select the best feature at each step.\n",
    "\n",
    "# Step 1: Selecting the Best Feature for Splitting:\n",
    "# The goal is to find the feature that best separates the classes. We use impurity measures such as:\n",
    "# Gini Impurity\n",
    "# Entropy (Information Gain)\n",
    "\n",
    "# Gini Impurity\n",
    "# Gini measures the probability that a randomly chosen element is incorrectly classified if it is randomly labeled.\n",
    "# Gini(D) = ∑(pi^2) , where pi = Probability of class i in dataset D, i = [1 to c] and c = number of classes\n",
    "\n",
    "# Example Calculation\n",
    "# If a dataset has 80% Pass (1) and 20% Fail (0):\n",
    "# Gini = 1 - (0.8 ^ 2 + 0.2 ^ 2) = 1 - (0.64 + 0.04) = 0.32\n",
    "# A lower Gini value means better purity.\n",
    "\n",
    "# Entropy (Information Gain)\n",
    "# Entropy measures the disorder in a dataset.\n",
    "# Entropy(D) = -(p_positive)*log(p_positive) - (p_negative)*log(p_negative) , Base is 2\n",
    "# Information Gain (IG) tells us how much entropy is reduced when splitting on a feature:\n",
    "# IG = H(S) - ∑ |Sv|/|S| * H(Sv)\n",
    "# where, H(S): Entropy of the root nood and v belongs to Value\n",
    "\n",
    "# Step 2: Recursive Splitting\n",
    "# The tree recursively splits nodes using the best feature.\n",
    "# The process stops when:\n",
    "# Nodes are pure (contain only one class).\n",
    "# Stopping criteria (e.g. max depth) is met.\n",
    "\n",
    "# Step 3: Making Predictions\n",
    "# For a new data point, the model:\n",
    "# Traverses the tree based on feature values.\n",
    "# Reaches a leaf node with a class label.\n",
    "\n",
    "# Step 4: Pruning (Avoiding Overfitting)\n",
    "# Pre-Pruning: Limit tree depth, min samples per leaf.\n",
    "# Post-Pruning: Remove nodes with low impact on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Using a Decision Tree Classifier for Binary Classification\n",
    "# A Decision Tree Classifier is an effective algorithm for binary classification, where the target variable \n",
    "# has only two classes (e.g. \"Yes\" vs \"No\"). It works by recursively splitting the dataset into homogeneous groups\n",
    "# using a set of decision rules.\n",
    "\n",
    "# Step 1: Data Preparation:\n",
    "# Collect labeled training data with features (independent variables) and a binary target variable.\n",
    "\n",
    "# Step 2: Selecting the Best Feature for Splitting:\n",
    "# The tree chooses a feature to split on using Gini Impurity or Entropy.\n",
    "\n",
    "# Step 3: Recursive Splitting:\n",
    "# The algorithm repeats the process, splitting nodes further until:\n",
    "# Nodes become pure (all samples belong to one class).\n",
    "# A stopping criterion is met (e.g. max depth, min samples per leaf).\n",
    "\n",
    "# Step 4: Making Predictions\n",
    "# For a new data point:\n",
    "# Start at the root node then go to the child node.\n",
    "\n",
    "# Step 5: Preventing Overfitting\n",
    "# To avoid overfitting, we can:\n",
    "# Limit tree depth (e.g. max_depth=3).\n",
    "# Set a minimum number of samples per leaf.\n",
    "# Use pruning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "# predictions.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Geometric Intuition Behind Decision Tree Classification:\n",
    "# A Decision Tree Classifier partitions the feature space into rectangular regions, making classification\n",
    "# decisions based on feature splits. This forms a hierarchical decision boundary, where each split divides the\n",
    "# space into smaller subregions.\n",
    "\n",
    "# Process of Decision Trees To Make Predictions Geometrically:\n",
    "# A new data point is placed into the feature space.\n",
    "# It follows the splitting rules (decision boundaries) to reach a region.\n",
    "# The majority class in that region is assigned as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "# classification model.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model. It shows the \n",
    "# counts of true positive, true negative, false positive, and false negative predictions. It's a powerful tool \n",
    "# for understanding not just how well a model is doing, but where it's making mistakes.   \n",
    "\n",
    "# Summary of confusion matrix:\n",
    "# The confusion matrix provides a much more detailed picture of model performance than simple accuracy.\n",
    "\n",
    "# Overall Accuracy:  While not the only important metric, we can calculate accuracy from the confusion matrix:\n",
    "# Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Precision:  Out of all the instances the model predicted as positive, how many were actually positive?\n",
    "# Precision = TP / (TP + FP)\n",
    "\n",
    "# Recall (Sensitivity or True Positive Rate): Out of all the actual positive instances, how many did the model \n",
    "# correctly identify?\n",
    "# Recall = TP / (TP + FN)\n",
    "\n",
    "# Specificity (True Negative Rate): Out of all the actual negative instances, how many did the model correctly identify?\n",
    "# Specificity = TN / (TN + FP)\n",
    "\n",
    "# F1-Score: The harmonic mean of precision and recall. Useful when we want to balance precision and recall, \n",
    "# especially in imbalanced datasets.\n",
    "# F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "# Understanding Errors: The confusion matrix help us to understand the types of errors our model is making.\n",
    "# Are we getting a lot of false positives or false negatives? This information is crucial for improving our model.\n",
    "\n",
    "# For example:   \n",
    "\n",
    "# High FP: The model is too eager to predict positive. We might need to adjust the classification threshold \n",
    "# or add more features.\n",
    "# High FN: The model is missing a lot of actual positives. We might need to adjust the classification threshold,\n",
    "# or we can use a different model, or gather more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "# calculated from it.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Example: Let’s say we have a model predicting whether an email is Spam (1) or Not Spam (0).\n",
    "# We have the confusion matrix data like, TP = 50, FN = 10, FP = 5 and TN = 100\n",
    "\n",
    "# Precision: TP / TP + FP = 50 / 50+5 = 0.909 = 90.9%\n",
    "# Recall: TP / TP + FN = 50 / 50+10 = 0.833 = 83.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "# explain how this can be done.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Understand the Classification Problem Type\n",
    "# Binary Classification: Two possible classes (e.g. spam vs. not spam).\n",
    "# Multi-class Classification: More than two classes (e.g. digit recognition 0-9).\n",
    "# Imbalanced Classification: When one class significantly outnumbers others (e.g. fraud detection).\n",
    "\n",
    "# 2. Consider the Consequences of False Positives & False Negatives\n",
    "# If False Positives (FP) are costly (e.g. medical diagnosis), then we need to focus on Precision.\n",
    "# If False Negatives (FN) are costly (e.g. fraud detection), then we need to focus on Recall.\n",
    "# If both FP and FN matter equally, we can use F1-score (harmonic mean of Precision & Recall).\n",
    "\n",
    "# 3. Choose the Right Metric Based on the Objective\n",
    "# Accuracy: Good for balanced datasets but misleading for imbalanced datasets.\n",
    "# Precision (Positive Predictive Value): Useful when FP needs to be minimized.\n",
    "# Recall (Sensitivity, True Positive Rate): Important when FN needs to be minimized.\n",
    "# F1-Score: A balanced metric when Precision and Recall are both important.\n",
    "# ROC-AUC (Receiver Operating Characteristic - Area Under Curve): Measures the model's ability to \n",
    "# distinguish between classes, useful for probabilistic classifiers.\n",
    "# PR-AUC (Precision-Recall Area Under Curve): Better than ROC-AUC when dealing with imbalanced datasets.\n",
    "# Log Loss: Measures how well the predicted probabilities match actual classes, useful for probabilistic classifiers.\n",
    "# Cohen's Kappa & Matthews Correlation Coefficient (MCC): More robust for imbalanced datasets.\n",
    "\n",
    "# 4. Check Business Goals & Interpretability\n",
    "# In medical applications, high recall is crucial.\n",
    "# In spam filtering, high precision is better.\n",
    "# In fraud detection, PR-AUC or MCC may be preferable.\n",
    "\n",
    "# Conclusion:\n",
    "# For balanced datasets, Accuracy or F1-score can work well.\n",
    "# For imbalanced datasets, Precision, Recall, F1-score, or MCC are better.\n",
    "# When probabilities matter, we can use ROC-AUC, PR-AUC, or Log Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "# explain why.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Medical Diagnosis for Cancer (Positive = Cancer, Negative = No Cancer)\n",
    "\n",
    "# Why Precision Matters?\n",
    "# A False Positive (FP) means diagnosing a healthy person with cancer, leading to unnecessary anxiety, \n",
    "# costly treatments, and side effects.\n",
    "# High precision ensures fewer false positives, meaning only actual cancer patients are flagged.\n",
    "# Ideal scenario: We want high precision so that only those truly having cancer are diagnosed as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# mail Spam Detection (Positive = Spam, Negative = Not Spam)\n",
    "\n",
    "# Why Recall Matters?\n",
    "# A False Negative (FN) means missing an actual spam email, which could result in phishing attacks or malware exposure.\n",
    "# High recall ensures most spam emails are correctly detected, even if some harmless emails get flagged as spam (FPs).\n",
    "# Ideal scenario: We prioritize high recall to catch as much spam as possible, even if it means filtering some \n",
    "# legitimate emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
